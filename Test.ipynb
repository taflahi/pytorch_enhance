{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.args['train_scales'] = 2\n",
    "dataloader.args['train'] = 'data/*.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_shape': 192,\n",
       " 'batch_size': 15,\n",
       " 'buffer_fraction': 5,\n",
       " 'buffer_size': 1500,\n",
       " 'train': 'data/*.jpg',\n",
       " 'train_blur': None,\n",
       " 'train_jpeg': [],\n",
       " 'train_noise': None,\n",
       " 'train_scales': 2,\n",
       " 'zoom': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dataloader.DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_size = 192 // 2\n",
    "images = np.zeros((15, 3, 192, 192), dtype=np.float32)\n",
    "seeds = np.zeros((15, 3, seed_size, seed_size), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.copy(images, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15, 3, 192, 192), (15, 3, 96, 96))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, seeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.3392157 , -0.3392157 , -0.33529413, ..., -0.00588235,\n",
       "         -0.06470588, -0.07647058],\n",
       "        [-0.35882354, -0.35882354, -0.32745099, ...,  0.10392159,\n",
       "          0.07647061,  0.0411765 ],\n",
       "        [-0.34705883, -0.3392157 , -0.32745099, ...,  0.08039218,\n",
       "          0.1156863 ,  0.11960787],\n",
       "        ..., \n",
       "        [-0.31176472, -0.32352942, -0.31960785, ..., -0.10784313,\n",
       "         -0.07254902, -0.04509804],\n",
       "        [-0.31960785, -0.31176472, -0.32745099, ..., -0.09999999,\n",
       "         -0.08823529, -0.06470588],\n",
       "        [-0.31568629, -0.31960785, -0.31960785, ..., -0.09607843,\n",
       "         -0.08823529, -0.10784313]],\n",
       "\n",
       "       [[-0.28039217, -0.28039217, -0.2764706 , ...,  0.13921571,\n",
       "          0.08431375,  0.07254905],\n",
       "        [-0.30000001, -0.30000001, -0.26862746, ...,  0.23725492,\n",
       "          0.21764708,  0.18235296],\n",
       "        [-0.28823531, -0.28039217, -0.26862746, ...,  0.21764708,\n",
       "          0.25294119,  0.25686276],\n",
       "        ..., \n",
       "        [-0.21372548, -0.22156861, -0.22156861, ...,  0.08039218,\n",
       "          0.1156863 ,  0.14313728],\n",
       "        [-0.22549018, -0.22156861, -0.23333332, ...,  0.09215689,\n",
       "          0.10000002,  0.12352943],\n",
       "        [-0.21764705, -0.22156861, -0.22549018, ...,  0.09215689,\n",
       "          0.10000002,  0.08039218]],\n",
       "\n",
       "       [[-0.15098038, -0.15098038, -0.14705881, ...,  0.30000001,\n",
       "          0.25294119,  0.24901962],\n",
       "        [-0.17058823, -0.17058823, -0.13921568, ...,  0.37843138,\n",
       "          0.36274511,  0.33137256],\n",
       "        [-0.16274509, -0.15098038, -0.13921568, ...,  0.34705883,\n",
       "          0.38627452,  0.39411765],\n",
       "        ..., \n",
       "        [-0.04117647, -0.05294117, -0.0490196 , ...,  0.2647059 ,\n",
       "          0.29607844,  0.31568629],\n",
       "        [-0.05294117, -0.04509804, -0.06078431, ...,  0.28039217,\n",
       "          0.28823531,  0.30784315],\n",
       "        [-0.05294117, -0.06078431, -0.05294117, ...,  0.28431374,\n",
       "          0.29215688,  0.26862746]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 384, 384])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = nn.PixelShuffle(2)\n",
    "input = autograd.Variable(torch.Tensor(1, 64, 192, 192))\n",
    "output = ps(input)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the nets\n",
    "class BasicLayer(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, kernel=3, stride=1, pad=1, alpha=0.25):\n",
    "        super(BasicLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channel, output_channel, kernel, stride=stride, padding=pad)\n",
    "        self.prelu = nn.PReLU(init=alpha)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        return self.prelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = BasicLayer(64, 6, 5, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 96, 96])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = autograd.Variable(torch.Tensor(1, 64, 192, 192))\n",
    "output = layer(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockLayer(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super(ResidualBlockLayer, self).__init__()\n",
    "        self.basic_layer = BasicLayer(input_channel, input_channel, 3, 1, 1, 0.1)\n",
    "    def forward(self, input):\n",
    "        x = self.basic_layer(input)\n",
    "        return torch.add(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 192, 192])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_layer = ResidualBlockLayer(64)\n",
    "output = res_layer(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.residual_size = 4\n",
    "        \n",
    "        # init\n",
    "        self.init_layer = BasicLayer(3, 64, kernel=7, pad=3)\n",
    "        \n",
    "        # residual layers\n",
    "        self.block_layer = []\n",
    "        for i in range(self.residual_size):\n",
    "            self.block_layer.append(ResidualBlockLayer(64))\n",
    "        \n",
    "        # upscale layers\n",
    "        self.upscale_layer = BasicLayer(64, 64 * 4)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(2)\n",
    "        \n",
    "        self.output_layer = nn.Conv2d(64, 3, 7, padding=3)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.init_layer(input)\n",
    "        for i in range(self.residual_size):\n",
    "            x = self.block_layer[i](x)\n",
    "        \n",
    "        x = self.upscale_layer(x)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 192, 192])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator()\n",
    "input = autograd.Variable(torch.from_numpy(seeds[:1]))\n",
    "gen_output = gen(input)\n",
    "gen_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preload vgg19\n",
    "from torchvision.models import vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace)\n",
       "    (18): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace)\n",
       "    (27): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace)\n",
       "    (36): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_pretrained = vgg.vgg19(pretrained=True)\n",
    "vgg_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        \n",
    "        self.lambd = lambd\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.lambd(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 192, 192])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset = np.array([103.939, 116.779, 123.680], dtype=np.float32).reshape((1,3,1,1))\n",
    "offset = autograd.Variable(torch.from_numpy(offset), requires_grad=False)\n",
    "lambd = LambdaLayer(lambda x: ((x+0.5)*255.0) - offset)\n",
    "input = autograd.Variable(torch.Tensor(1, 3, 192, 192))\n",
    "output = lambd(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptual(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptual, self).__init__()\n",
    "        \n",
    "        offset_ = np.array([103.939, 116.779, 123.680], dtype=np.float32).reshape((1,3,1,1))\n",
    "        self.offset = autograd.Variable(torch.from_numpy(offset_), requires_grad=False)\n",
    "        self.lambd = lambd = LambdaLayer(lambda x: ((x+0.5)*255.0) - self.offset)\n",
    "        \n",
    "        # init with pretrained vgg19\n",
    "        original = vgg.vgg19(pretrained=True)\n",
    "        self.features = list(original.features.children())[:32]\n",
    "    \n",
    "    def forward(self, input):\n",
    "        conv_1_2, conv_2_2, conv_3_2 = None, None, None\n",
    "        x = self.lambd(input)\n",
    "        for i in range(len(self.features)):\n",
    "            x = self.features[i](x)\n",
    "            if i == 3:\n",
    "                conv_1_2 = x.clone()\n",
    "            elif i == 7:\n",
    "                conv_2_2 = x.clone()\n",
    "            elif i == 11:\n",
    "                conv_3_2 = x.clone()\n",
    "        \n",
    "        return conv_1_2, conv_2_2, conv_3_2, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 192, 192]),\n",
       " torch.Size([1, 128, 96, 96]),\n",
       " torch.Size([1, 256, 48, 48]),\n",
       " torch.Size([1, 512, 12, 12]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc = Perceptual()\n",
    "input = autograd.Variable(torch.Tensor(1, 3, 192, 192))\n",
    "conv_1_2, conv_2_2, conv_3_2, conv_5_4 = perc(input)\n",
    "conv_1_2.shape, conv_2_2.shape, conv_3_2.shape, conv_5_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.input_channel = 64\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm2d(self.input_channel)\n",
    "        self.conv_layer1_1 = BasicLayer(self.input_channel, self.channels, 5, 2, 2)\n",
    "        self.conv_layer1_2 = BasicLayer(self.channels, self.channels, 5, 2, 2)\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm2d(2 * self.input_channel)\n",
    "        self.conv_layer2 = BasicLayer(2 * self.input_channel, 2 * self.channels, 5, 2, 2)\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm2d(4 * self.input_channel)\n",
    "        self.conv_layer3 = BasicLayer(4 * self.input_channel, 3 * self.channels, 3, 1, 1)\n",
    "        \n",
    "        self.conv_layer4 = BasicLayer(6 * self.channels, 4 * self.channels, 1, 1, 0)\n",
    "        self.conv_layer5 = BasicLayer(4 * self.channels, 3 * self.channels, 3, stride=2)\n",
    "        self.conv_layer6 = BasicLayer(3 * self.channels, 2 * self.channels, 1, 1, 0)\n",
    "        \n",
    "        self.batch_norm7 = nn.BatchNorm2d(2 * self.channels)\n",
    "        self.conv_layer7 = nn.Conv2d(2 * self.channels, 1, 1)\n",
    "    \n",
    "    def forward(self, conv_1_2, conv_2_2, conv_3_2):\n",
    "        x1 = self.batch_norm1(conv_1_2)\n",
    "        x1 = self.conv_layer1_1(x1)\n",
    "        x1 = self.conv_layer1_2(x1)\n",
    "        \n",
    "        x2 = self.batch_norm2(conv_2_2)\n",
    "        x2 = self.conv_layer2(x2) \n",
    "        \n",
    "        x3 = self.batch_norm3(conv_3_2)\n",
    "        x3 = self.conv_layer3(x3) \n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        print(x1.shape, x2.shape, x3.shape, x.shape)\n",
    "        \n",
    "        x = self.conv_layer4(x)\n",
    "        x = self.conv_layer5(x)\n",
    "        x = self.conv_layer6(x)\n",
    "        \n",
    "        x = self.batch_norm7(x)\n",
    "        return self.conv_layer7(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 48, 48]) torch.Size([1, 64, 48, 48]) torch.Size([1, 96, 48, 48]) torch.Size([1, 192, 48, 48])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 24, 24])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc = Discriminator(32)\n",
    "disc_output = disc(conv_1_2.detach(), conv_2_2.detach(), conv_3_2.detach())\n",
    "disc_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(input, target):\n",
    "    return torch.sum((input - target) ** 2) / input.data.nelement()\n",
    "\n",
    "def loss_perceptual(input, target):\n",
    "    return mse_loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_total_variation(x):\n",
    "    return torch.mean(((x[:,:,:-1,:-1] - x[:,:,1:,:-1])**2 + (x[:,:,:-1,:-1] - x[:,:,:-1,1:])**2)**1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.8149\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.8149\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_5_4 = conv_5_4.detach()\n",
    "# target_5_4.requires_grad = False\n",
    "\n",
    "out1 = loss_perceptual(conv_5_4, target_5_4 * 0.5)\n",
    "out2 = loss_total_variation(gen_output)\n",
    "output2 = out1 + out2\n",
    "print(output2)\n",
    "output2.backward()\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-5\n",
      "-1\n",
      " 0\n",
      " 1\n",
      " 5\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      "-5.0067\n",
      "-1.3133\n",
      "-0.6931\n",
      "-0.3133\n",
      "-0.0067\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      " 0.0067\n",
      " 0.3133\n",
      " 0.6931\n",
      " 1.3133\n",
      " 5.0067\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def softminus(x):\n",
    "    return x - F.softplus(x)\n",
    "\n",
    "tx = autograd.Variable(torch.Tensor([-5, -1, 0, 1, 5]))\n",
    "print(tx)\n",
    "print(softminus(tx))\n",
    "print(F.softplus(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.4666\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_adversarial(input):\n",
    "    return torch.mean(1 - softminus(input))\n",
    "\n",
    "tx = autograd.Variable(torch.Tensor([-5, -1, 0, 1, 5]))\n",
    "output = loss_adversarial(tx)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.3813\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_discriminator(input, target):\n",
    "    return torch.mean(softminus(input) - F.softplus(target))\n",
    "\n",
    "i = autograd.Variable(torch.Tensor([-5, -1, 0, 1, 5]))\n",
    "t = autograd.Variable(torch.Tensor([-2, -1, 0, 1, 2]))\n",
    "output = loss_discriminator(i, t)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct full network\n",
    "class Enhancer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Enhancer, self).__init__()\n",
    "        \n",
    "        self.generator = Generator()\n",
    "        self.perceptual = Perceptual()\n",
    "        self.discriminator = Discriminator(32)\n",
    "    \n",
    "    def create_new_discriminator(size):\n",
    "        self.discriminator = Discriminator(size)\n",
    "    \n",
    "    def forward(self, inputs, seeds):\n",
    "        inputs = autograd.Variable(torch.from_numpy(inputs))\n",
    "        seeds = autograd.Variable(torch.from_numpy(seeds))\n",
    "        \n",
    "        gen_out = self.generator(seeds)\n",
    "        c12, c22, c32, perc_out = self.perceptual(torch.cat([inputs, gen_out], dim=0))\n",
    "        disc_out = self.discriminator(c12, c22, c32)\n",
    "        \n",
    "        return gen_out, c12, c22, c32, perc_out, disc_out\n",
    "    \n",
    "    def discriminator_clone(self):\n",
    "        disc = Discriminator(self.discriminator.channels)\n",
    "\n",
    "        mp = list(disc.parameters())\n",
    "        mcp = list(self.discriminator.parameters())\n",
    "        n = len(mp)\n",
    "        for i in range(0, n):\n",
    "            mp[i].data[:] = mcp[i].data[:]\n",
    "            \n",
    "        return disc\n",
    "    \n",
    "    def assign_back_discriminator(self, disc):\n",
    "        mp = list(self.discriminator.parameters())\n",
    "        mcp = list(disc.parameters())\n",
    "        n = len(mp)\n",
    "        for i in range(0, n):\n",
    "            mp[i].data[:] = mcp[i].data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 192, 192])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1 = autograd.Variable(torch.Tensor(1, 3, 192, 192))\n",
    "i2 = autograd.Variable(torch.Tensor(1, 3, 192, 192))\n",
    "torch.cat([i1, i2], dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhancer = Enhancer()\n",
    "gen_out, c12, c22, c32, c52, disc_out = enhancer(images[:1], seeds[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone discriminator on the full network\n",
    "disc = enhancer.discriminator_clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_out2 = disc(c12.detach(), c22.detach(), c32.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = optim.Adam(enhancer.generator.parameters(), lr = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get current lr\n",
    "lr = -1000\n",
    "for param_group in optimizer1.param_groups:\n",
    "    lr = param_group['lr']\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate_for_step(optimizer1, lr, 75, 0.5, 0)\n",
    "\n",
    "# get updated lr\n",
    "for param_group in optimizer1.param_groups:\n",
    "    lr = param_group['lr']\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1922.5225\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adversarial loss\n",
    "# loss perceptual using c22 for pretrain for 50 epochs, or using c52 for actual training for 250 epochs\n",
    "gen_loss = loss_perceptual(c52[:1], c52[1:]) * 1e0 \\\n",
    "    + loss_total_variation(gen_out) * 2e5 \\\n",
    "    + loss_adversarial(disc_out[1:]) * 5e2\n",
    "gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.3808\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc_loss = loss_discriminator(disc_out2[:1], disc_out2[1:])\n",
    "disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update both networks then copy back disc value to enchancer\n",
    "enhancer.assign_back_discriminator(disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain\n",
    "# --smoothness-weight=1e7 --adversary-weight=0.0\n",
    "# add_arg('--generator-start',    default=0, type=int,                help='Epoch count to start training generator.')\n",
    "# add_arg('--discriminator-start',default=1, type=int,                help='Epoch count to update the discriminator.')\n",
    "# add_arg('--adversarial-start',  default=2, type=int,                help='Epoch for generator to use discriminator.')\n",
    "# add_arg('--perceptual-weight',  default=1e0, type=float,            help='Weight for VGG-layer perceptual loss.')\n",
    "# add_arg('--smoothness-weight',  default=2e5, type=float,            help='Weight of the total-variation loss.')\n",
    "# add_arg('--adversary-weight',   default=5e2, type=float,            help='Weight of adversarial loss compoment.')\n",
    "# train\n",
    "# --smoothness-weight=2e4 --adversary-weight=1e3 --generator-start=5 --discriminator-start=0 --adversarial-start=5 \n",
    "# --discriminator-size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decay learning rate\n",
    "def learning_rate_for_step(optimizer, current_lr, lr_period, decay_lr, step):\n",
    "    l_r, t_cur = current_lr, step\n",
    "    if t_cur % lr_period == 0: \n",
    "        l_r *= decay_lr\n",
    "        \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = l_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
